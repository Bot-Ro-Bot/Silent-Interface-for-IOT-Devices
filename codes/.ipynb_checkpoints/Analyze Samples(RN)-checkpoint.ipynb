{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the Extracted Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "mlp.rc(\"xtick\",labelsize=12)\n",
    "mlp.rc(\"ytick\",labelsize=12)\n",
    "mlp.rc(\"axes\",labelsize=14)\n",
    "plt.rcParams[\"figure.figsize\"] = [10,5]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "CURR_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make necessary definitions\n",
    "\n",
    "MAIN_DIR = os.path.dirname(CURR_DIR)\n",
    "DATA_DIR = os.path.join(MAIN_DIR,\"dataset\")\n",
    "FIG_DIR = os.path.join(MAIN_DIR,\"figures\")\n",
    "os.makedirs(FIG_DIR,exist_ok=True)\n",
    "\n",
    "SPEAKER = [\"RL\",\"RN\",\"SR\",\"US\"]\n",
    "SESSION = [\"session1\",\"session2\",\"session3\"]\n",
    "MODE = [\"mentally\",\"mouthed\",\"audible\"]\n",
    "SENTENCES =[\"अबको समय सुनाउ\",\"एउटा सङ्गित बजाउ\",\"आजको मौसम बताउ\",\"बत्तिको अवस्था बदल\",\"पङ्खाको स्तिथी बदल\"]\n",
    "WORDS = [\"समय\",\"सङ्गित\",\"मौसम\",\"बत्ति\",\"पङ्खा\"]\n",
    "\n",
    "SENTENCE_LABEL=[0,1,2,0,3,1,0,3,0,0,1,1,3,3,4,4,2,3,1,2,2,2,4,4,4]\n",
    "WORD_LABEL=[4,0,3,1,0,1,1,0,4,0,3,2,4,4,2,1,4,1,2,2,2,0,3,3,3]\n",
    "\n",
    "LABELS = {\"words\":WORD_LABEL,\"sentences\":SENTENCE_LABEL}\n",
    "\n",
    "SAMPLING_RATE = 250\n",
    "\n",
    "# a function to save plotted figures\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=100):\n",
    "    path = os.path.join(FIG_DIR, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "/home/deadpool/github/Silent-Interface-for-IOT-Devices/dataset/RL/session3/sentences/mouthed/OpenBCI-RAW-2021-05-15_15-50-43.txt\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(DATA_DIR+\"/*/*/*/*/*.txt\",recursive=True)\n",
    "print(len(all_files))\n",
    "print(all_files[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-08ed8b7987e7>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-08ed8b7987e7>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    dataset = {\"data\":[], \"speaker\":[],\"session\":[], \"mode\":[], \"utter_type\":[],\"file_name\":[],\"labels\"}\u001b[0m\n\u001b[0m                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def parser(files):\n",
    "    \"\"\"\n",
    "    parser function to extract utterances from .txt file and store them in a dictionary\n",
    "    \"\"\"\n",
    "    dataset = {\"data\":[], \"speaker\":[],\"session\":[], \"mode\":[], \"utter_type\":[],\"file_name\":[],\"labels\":[]}\n",
    "    def get_data(file):\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "        mode = file.split(\"/\")[-2]\n",
    "        utter_type = file.split(\"/\")[-3]\n",
    "        session = file.split(\"/\")[-4]\n",
    "        speaker = file.split(\"/\")[-5]\n",
    "        f = open(file, 'r')\n",
    "        contents = map(lambda x : x.strip(), f.readlines())\n",
    "        #the file starts with '%' and some instruction before data and removing these data \n",
    "        frames_original = list(filter(lambda x : x and x[0] != '%', contents))[1:]\n",
    "        print(\"frames_original:\", frames_original)\n",
    "        #the data row contains channels info digital trigger and accelerometer info separated by comma\n",
    "        frames_original = list(map(lambda s : list(map( lambda ss: ss.strip(), s.split(','))), frames_original))\n",
    "        # (8 channels) + digital triggers\n",
    "        # the digital trigger is in a[16], used to indicate the utterance\n",
    "        frames = list(map(lambda a: list(map(float, a[1:9])) + [float(a[16])] , frames_original))\n",
    "        frames = np.array(frames)\n",
    "        indices = []\n",
    "        signal = []\n",
    "        for index,f in enumerate(frames[:,-1]):\n",
    "            if(bool(f) ^ bool(frames[(index+1) if ((index+1)<len(frames)) else index,-1]) ):\n",
    "                indices.append(index)\n",
    "                if len(indices)>1 and len(indices)%2==0:\n",
    "                    signal.append(frames[indices[len(indices)-2]:indices[len(indices)-1],:-1])   \n",
    "        \n",
    "        dataset[\"data\"].extend(signal)\n",
    "        dataset[\"speaker\"].extend([speaker]*len(signal))\n",
    "        dataset[\"session\"].extend([session]*len(signal))\n",
    "        dataset[\"mode\"].extend([mode]*len(signal))\n",
    "        dataset[\"utter_type\"].extend([utter_type]*len(signal))\n",
    "        dataset[\"file_name\"].extend([file_name]*len(signal))\n",
    "        dataset[\"labels\"].extend(LABELS[utter_type])\n",
    "        \n",
    "#     for file,i in zip(files,tqdm.tqdm(range(1,len(files)+1),desc=\"PARSING DATA\")):\n",
    "#         get_data(file)\n",
    "#         time.sleep(0.5)\n",
    "    \n",
    "    for file in files:\n",
    "        get_data(file)\n",
    "#         time.sleep(1)\n",
    "        \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-fd6f6887140e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-8b73bfec54e2>\u001b[0m in \u001b[0;36mparser\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#         time.sleep(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-8b73bfec54e2>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"frames_original:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#the data row contains channels info digital trigger and accelerometer info separated by comma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mframes_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# (8 channels) + digital triggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# the digital trigger is in a[16], used to indicate the utterance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-8b73bfec54e2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"frames_original:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#the data row contains channels info digital trigger and accelerometer info separated by comma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mframes_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# (8 channels) + digital triggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# the digital trigger is in a[16], used to indicate the utterance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-8b73bfec54e2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(ss)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"frames_original:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#the data row contains channels info digital trigger and accelerometer info separated by comma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mframes_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# (8 channels) + digital triggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# the digital trigger is in a[16], used to indicate the utterance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_data = parser(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data[\"data\"][0].shape\n",
    "all_data[\"data\"][1].shape\n",
    "# all_data[\"data\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>speaker</th>\n",
       "      <th>mode</th>\n",
       "      <th>utter_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-24482.6015625, 7188.3876953125, 1742.787841...</td>\n",
       "      <td>US</td>\n",
       "      <td>mentally</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-24702.41015625, 6931.9462890625, 1489.98962...</td>\n",
       "      <td>US</td>\n",
       "      <td>mentally</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-24613.494140625, 7119.65625, 1720.257202148...</td>\n",
       "      <td>US</td>\n",
       "      <td>mentally</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-24671.564453125, 6960.1318359375, 1552.4851...</td>\n",
       "      <td>US</td>\n",
       "      <td>mentally</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-24671.4296875, 7121.73486328125, 1765.76538...</td>\n",
       "      <td>US</td>\n",
       "      <td>mentally</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>[[-192.94024658203125, -10373.7568359375, -168...</td>\n",
       "      <td>RL</td>\n",
       "      <td>audible</td>\n",
       "      <td>sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>[[-218.64476013183594, -10277.287109375, -1516...</td>\n",
       "      <td>RL</td>\n",
       "      <td>audible</td>\n",
       "      <td>sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>[[-316.6347961425781, -10496.892578125, -1831....</td>\n",
       "      <td>RL</td>\n",
       "      <td>audible</td>\n",
       "      <td>sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>[[-439.14471435546875, -10457.419921875, -1464...</td>\n",
       "      <td>RL</td>\n",
       "      <td>audible</td>\n",
       "      <td>sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>[[-495.1805419921875, -10950.72265625, -1888.9...</td>\n",
       "      <td>RL</td>\n",
       "      <td>audible</td>\n",
       "      <td>sentences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4009 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   data speaker      mode  \\\n",
       "0     [[-24482.6015625, 7188.3876953125, 1742.787841...      US  mentally   \n",
       "1     [[-24702.41015625, 6931.9462890625, 1489.98962...      US  mentally   \n",
       "2     [[-24613.494140625, 7119.65625, 1720.257202148...      US  mentally   \n",
       "3     [[-24671.564453125, 6960.1318359375, 1552.4851...      US  mentally   \n",
       "4     [[-24671.4296875, 7121.73486328125, 1765.76538...      US  mentally   \n",
       "...                                                 ...     ...       ...   \n",
       "4004  [[-192.94024658203125, -10373.7568359375, -168...      RL   audible   \n",
       "4005  [[-218.64476013183594, -10277.287109375, -1516...      RL   audible   \n",
       "4006  [[-316.6347961425781, -10496.892578125, -1831....      RL   audible   \n",
       "4007  [[-439.14471435546875, -10457.419921875, -1464...      RL   audible   \n",
       "4008  [[-495.1805419921875, -10950.72265625, -1888.9...      RL   audible   \n",
       "\n",
       "     utter_type  \n",
       "0          word  \n",
       "1          word  \n",
       "2          word  \n",
       "3          word  \n",
       "4          word  \n",
       "...         ...  \n",
       "4004  sentences  \n",
       "4005  sentences  \n",
       "4006  sentences  \n",
       "4007  sentences  \n",
       "4008  sentences  \n",
       "\n",
       "[4009 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_name = df[\"file_name\"]\n",
    "print(len(all_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = df[df[\"utter_type\"]==\"words\"]\n",
    "all_sentences = df[df[\"utter_type\"]==\"sentences\"]\n",
    "all_word_length = [x.shape[0] for x in all_words[\"data\"]]\n",
    "all_sent_length = [x.shape[0] for x in all_sentences[\"data\"]]\n",
    "\n",
    "all_words_audible = df.query('utter_type==\"words\" and mode==\"audible\"')\n",
    "all_sent_audible = df.query('utter_type==\"sentences\" and mode==\"audible\"')\n",
    "\n",
    "\n",
    "print(len(all_words))\n",
    "print(len(all_sentences))\n",
    "#all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_utter = len(all_files)*25   # 125 utterances per file\n",
    "sent_utter = word_utter = total_utter*0.5\n",
    "# print(\"error word recordings: \", all_words-word_utter)\n",
    "# print(\"error sentence recordings: \", all_sentences-sent_utter)\n",
    "\n",
    "\n",
    "def error_files(X,Y,sec): # sec:minimun uttrance time: 1 for word and 2.5 for sentence\n",
    "    #err_length = []\n",
    "    err_position = []\n",
    "    err_file_name = []\n",
    "    count = 0\n",
    "    \n",
    "    for word in X:\n",
    "        if (word < (sec*250)):       \n",
    "            #err_length.append(word)\n",
    "            err_position.append(count)\n",
    "        count+=1\n",
    "   # print(len(err_word_length))\n",
    "   # print(len(err_word_position))\n",
    "   \n",
    "    for i in err_position:\n",
    "        err_file_name.append([Y[i]])\n",
    "\n",
    "    return(np.unique((err_file_name)),len(np.unique(err_file_name)))\n",
    "\n",
    "if (len(all_sentences)>sent_utter):    \n",
    "    print(\"error sentence file names:\",error_files(all_sent_length,all_file_name,2.5))\n",
    "# checking files for error word\n",
    "\n",
    "if (len(all_words)>word_utter):    \n",
    "    print(\"error word file names:\",error_files(all_word_length,all_file_name,1.58))\n",
    "    \n",
    "    \n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_sent_audible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_audible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_word_length,alpha=0.4)\n",
    "plt.hist(all_sent_length,alpha=0.5)\n",
    "plt.xlabel(\"Sample Length\")\n",
    "plt.ylabel(\"Samples\")\n",
    "# save_fig(\"length distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max length of words utterances in seconds: \",np.max(all_word_length)/SAMPLING_RATE)\n",
    "print(\"Max length of sentence utterances in seconds: \",np.max(all_sent_length)/SAMPLING_RATE)\n",
    "print(\"Min length of words utterances in seconds: \",np.min(all_word_length)/SAMPLING_RATE)\n",
    "print(\"Min length of sentence utterances in seconds: \",np.min(all_sent_length)/SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.transpose(all_data[\"data\"][1])\n",
    "print(all_data[\"speaker\"][1])\n",
    "print(all_data[\"mode\"][1])\n",
    "print(sample.shape)\n",
    "\n",
    "sample2 = np.transpose(all_words_audible[\"data\"].iloc[20])\n",
    "print(all_words_audible[\"speaker\"].iloc[20])\n",
    "print(all_words_audible[\"mode\"].iloc[20])\n",
    "print(sample2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=8,ncols=1)\n",
    "for i in range(8):\n",
    "#     axes[i].plot(sample[i])\n",
    "    axes[i].plot(sample2[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=8,ncols=1)\n",
    "for i in range(8):\n",
    "    axes[i].plot(sample[i])\n",
    "#     axes[i].plot(sample2[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft,fftfreq\n",
    "from scipy.fftpack import fft\n",
    "from scipy.stats import zscore\n",
    "sample = np.transpose(sample)\n",
    "N = len(sample[:,0])\n",
    "y = fft(zscore(sample[:,0]))\n",
    "x = fftfreq(N,1/250)\n",
    "\n",
    "sample2 = np.transpose(sample2)\n",
    "N = len(sample2[:,0])\n",
    "Y = fft(zscore(sample2[:,0]))\n",
    "X = fftfreq(N,1/250)\n",
    "\n",
    "plt.plot(abs(x),abs(y))\n",
    "plt.plot(abs(X),abs(Y))\n",
    "plt.legend([\"mentally\",\"audible\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
